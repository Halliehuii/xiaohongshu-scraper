from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
import json
import time
import random
import os
import requests
from urllib.parse import urlparse
import re

def extract_xiaohongshu_url(input_text):
    """
    ä»æ‰‹æœºç«¯å¤åˆ¶çš„æ–‡æœ¬ä¸­æå–å°çº¢ä¹¦URL
    
    Args:
        input_text (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š
        "34 æ‹“éº»æ…§å­å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ğŸ˜† tnk9IKuwcqYQnJK ğŸ˜† http://xhslink.com/a/IGTNc5Db7WEab"
        
    Returns:
        str: æå–å‡ºçš„URLï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    # åŒ¹é… http://xhslink.com/a/ å¼€å¤´çš„å®Œæ•´URL
    pattern = r'http://xhslink\.com/a/[a-zA-Z0-9]+'
    match = re.search(pattern, input_text)
    
    if match:
        url = match.group(0)
        print(f"å·²æå–URL: {url}")
        return url
        
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°URLï¼Œæç¤ºç”¨æˆ·
    print("è­¦å‘Šï¼šæœªèƒ½ä»è¾“å…¥æ–‡æœ¬ä¸­æå–åˆ°æœ‰æ•ˆçš„å°çº¢ä¹¦é“¾æ¥")
    return None

class XiaohongshuScraper:
    def __init__(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        self.setup_driver()
        self.is_logged_in = False
        
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é€‰é¡¹"""
        chrome_options = Options()
        # chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œå¦‚æœä¸éœ€è¦çœ‹åˆ°æµè§ˆå™¨ç•Œé¢å¯ä»¥å–æ¶ˆæ³¨é‡Š
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--start-maximized')
        
        # æ·»åŠ æ›´çœŸå®çš„æµè§ˆå™¨ç‰¹å¾
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è®¾ç½®ä¸­æ–‡è¯­è¨€
        chrome_options.add_argument('--lang=zh-CN')
        
        # åˆå§‹åŒ–WebDriver
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # è®¾ç½®ç­‰å¾…æ—¶é—´
        self.wait = WebDriverWait(self.driver, 20)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°20ç§’
        
    def create_output_directory(self, title):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤éæ³•å­—ç¬¦
        clean_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        # é™åˆ¶æ ‡é¢˜é•¿åº¦
        clean_title = clean_title[:50]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join('xiaohongshu_posts', clean_title)
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
        
    def download_image(self, url, output_dir, index):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            path = parsed_url.path
            ext = os.path.splitext(path)[1]
            if not ext:
                ext = '.jpg'  # é»˜è®¤æ‰©å±•å
                
            # æ„å»ºæ–‡ä»¶å
            filename = f"{index:03d}{ext}"
            filepath = os.path.join(output_dir, filename)
            
            # ä¸‹è½½å›¾ç‰‡
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            print(f"å·²ä¸‹è½½å›¾ç‰‡: {filename}")
            return filepath
            
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            return None
            
    def download_images(self, image_urls, output_dir):
        """ä¸‹è½½æ‰€æœ‰å›¾ç‰‡"""
        downloaded_files = []
        for i, url in enumerate(image_urls, 1):
            filepath = self.download_image(url, output_dir, i)
            if filepath:
                downloaded_files.append(filepath)
        return downloaded_files
        
    def login(self):
        """ç™»å½•å°çº¢ä¹¦"""
        if self.is_logged_in:
            return True
            
        try:
            print("\nè¯·ç™»å½•å°çº¢ä¹¦...")
            self.driver.get('https://www.xiaohongshu.com')
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½• - åœ¨APIä¸­è¿™éœ€è¦ç”¨æˆ·é€šè¿‡æµè§ˆå™¨æ‰‹åŠ¨ç™»å½•
            print("ç­‰å¾…å°çº¢ä¹¦ç½‘ç«™åŠ è½½ï¼Œè¯·ç¨å...")
            time.sleep(5)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®
            try:
                # åˆ¤æ–­æ˜¯å¦å·²ç»ç™»å½•
                if "ç”¨æˆ·ç™»å½•" in self.driver.page_source or "ç™»å½•" in self.driver.page_source:
                    print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•ï¼Œæ­¤å¤„APIéœ€è¦å•ç‹¬å¤„ç†ç™»å½•æµç¨‹")
                    return False
                else:
                    # å·²ç»ç™»å½•æˆåŠŸ
                    self.is_logged_in = True
                    print("ç™»å½•æˆåŠŸï¼")
                    
                    # ä¿å­˜cookies
                    cookies = self.driver.get_cookies()
                    with open('xiaohongshu_cookies.json', 'w') as f:
                        json.dump(cookies, f)
                    
                    return True
            except Exception as e:
                print(f"æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
                return False
            
        except Exception as e:
            print(f"ç™»å½•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return False
            
    def load_cookies(self):
        """åŠ è½½å·²ä¿å­˜çš„cookies"""
        try:
            if os.path.exists('xiaohongshu_cookies.json'):
                with open('xiaohongshu_cookies.json', 'r') as f:
                    cookies = json.load(f)
                for cookie in cookies:
                    self.driver.add_cookie(cookie)
                return True
        except Exception as e:
            print(f"åŠ è½½cookiesæ—¶å‡ºé”™: {str(e)}")
        return False
        
    def scrape_post(self, url):
        """
        æŠ“å–å°çº¢ä¹¦å¸–å­å†…å®¹
        
        Args:
            url (str): å°çº¢ä¹¦å¸–å­URL
            
        Returns:
            dict: åŒ…å«æ ‡é¢˜ã€å†…å®¹å’Œå›¾ç‰‡URLçš„å­—å…¸
        """
        try:
            if not self.is_logged_in:
                # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookies
                if not self.load_cookies():
                    if not self.login():
                        return None
            
            print("æ­£åœ¨åŠ è½½é¡µé¢...")
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(random.uniform(3, 5))
            
            # ç­‰å¾…å†…å®¹åŠ è½½å®Œæˆ
            print("ç­‰å¾…å†…å®¹åŠ è½½...")
            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # è·å–é¡µé¢å†…å®¹
            print("æå–å†…å®¹...")
            
            try:
                # å°è¯•è·å–æ ‡é¢˜
                title_element = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.title, .title")))
                title = title_element.text
            except:
                title = "æœªæ‰¾åˆ°æ ‡é¢˜"
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æ ‡é¢˜")
            
            try:
                # å°è¯•è·å–æ­£æ–‡å†…å®¹
                content_element = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".content, .desc")))
                content = content_element.text
            except:
                content = "æœªæ‰¾åˆ°å†…å®¹"
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æ­£æ–‡å†…å®¹")
            
            # è·å–å›¾ç‰‡URL
            image_urls = []
            try:
                # ç­‰å¾…å›¾ç‰‡å…ƒç´ åŠ è½½
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "img[src*='http']")))
                images = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='http']")
                for img in images:
                    src = img.get_attribute('src')
                    if src and not src.startswith('data:'):
                        image_urls.append(src)
            except:
                print("è­¦å‘Šï¼šæå–å›¾ç‰‡URLæ—¶å‡ºé”™")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•å¹¶ä¸‹è½½å›¾ç‰‡
            output_dir = self.create_output_directory(title)
            downloaded_files = self.download_images(image_urls, output_dir)
            
            return {
                'title': title,
                'content': content,
                'image_urls': image_urls,
                'downloaded_files': downloaded_files,
                'output_dir': output_dir
            }
            
        except TimeoutException:
            print("é”™è¯¯ï¼šé¡µé¢åŠ è½½è¶…æ—¶")
            return None
        except Exception as e:
            print(f"é”™è¯¯ï¼šæŠ“å–è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}")
            return None
            
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()

def save_to_file(data, filename='xiaohongshu_content.json'):
    """å°†æŠ“å–çš„æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}") 